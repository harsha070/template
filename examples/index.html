<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
</head>

<body>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Control in Neural Speech Synthesis",
    "description": "",
    "authors": [
      {
        "author":"Sree Harsha Tanneru",
        "authorURL":"https://harsha070.github.io/",
        "affiliations": [{"name": "Independent"}]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <!--<figure style="grid-column: page; margin: 1rem 0;"><img src="momentum.png"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>-->
    <p>Speech Synthesis, also known as Text to Speech is the task of synthesizing natural and intelligible speech from text. Modern day TTS Systems
    involve expertise from Machine Learning, Linguistics and Signal Processing. Research on TTS has shifted from Early Concatenative Synthesis, to Parametric Synthesis to Neural Speech Synthesis.
      In this blog post, we give a high level overview of TTS Systems, and illustrate Why and How
    Speech Output can be controlled. We specifically look at how output waveform and attention vary with the explicit introduction of pitch and emotion features.</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-0" id="section-0"><span>0</span></a>
    <h2>Why Control ?</h2>
    Why Control ? In a traditional Machine Learning problem, generalization is achieved by throwing in more data. Unlike a traditional machine learning problem,
    Text-To-Speech is a one-to-many mapping problem. For a given text, the output could be speech with varying voice, pitch, emotion, energy and duration.
    Recent Research has demonstrated ability to control Duration, Pitch, Energy in the output speech. We first demonstrate how
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>Overview of Neural Text-to-Speech Synthesis</h2>
    <p>Text is first converted to linguistic features, and then acoustic features, followed by voice</p>
    <p>Text Analysis module converts a text sequence into linguistic features.</p>
    <p>Test for owner's possessive. Test for "quoting a passage." And another sentence. Or two. Some flopping fins; for
      diving.</p>
    <h3>Text Analysis</h3>
    <p>Text Analysis converts text into rich information about pronunciation, rhythm, stress, intonation and prosody. These lingusitc features serve as the input to acoustic models.
    Phonemes are one such way to repesent linguistic features. Phoneme is a perceptually distinct unit of sound.
      English language has 44 phones, meaning pronunciation of any english word can be represented with these 44 phones.
      <br>Example: the: <i>DH AHO</i>, dog: <i>D A01 G</i>
    </p>
    <figure style="grid-column: page; margin: 0rem 0;"><img src="text-analysis.gif"
        style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure>
    <h3>Acoustic Model</h3>
    Acoustic Models, are Sequence-To-Sequence models, to generate acoustic features from linguistic features, which are converted to waveform downstream.
    The kind of input linguistic features and output acoustic feature decide the type of model. Mel-Capstral Coefficients, Mel-Generalized Coefficients, Linear Spectrograms, Mel-Spectrograms are examples of acoustic features.
    Mel-Spectrogram is a widely used acoustic feature output, owing to it's minimal information loss.
    <h4>Mel-Spectrogram Calculaton</h4>
    <p>Here's a test of an inline equation <d-math>c = a^2 + b^2</d-math>. Also with configurable katex standards just
      using inline '$' signs: $$x^2$$ And then there's a block equation:</p>
    <d-math block>
      c = \pm \sqrt{ \sum_{i=0}^{n}{a^{222} + b^2}}
    </d-math>
    <p>Math can also be quite involved:</p>
    <d-math block>
      \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} = 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}}
      {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } }
    </d-math>
    <figure style="grid-column: page; margin: 0rem 0;"><img src="ezgif.com-gif-maker.gif"
        style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure>
    <h3>Vocoders</h3>
    Vocoders are used to synthesize waveform from acoustic features. Neural Vocoders have recently gained popularity owing to their higher speech quality.
     Since speech waveform is very long, autoregressive waveform generation takes much inference time. Thus, generative models such as Flow, GAN,
    VAE, and DDPM (Denoising Diffusion Probabilistic Model, Diffusion for short) are used in waveform generation. Prominent TTS Methods used GAN based Vocoders, where Generators use transposed and dilated convolutions
    to generate the waveform, and Discriminator judges the authenticity of generated data.
    <figure style="grid-column: span; margin: 0rem 0; align-content: center"><img src="vocoder.png"
        style="width:70%; border: 0px solid rgba(0, 0, 0, 0.2); align-content: space-evenly" /></figure>
    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h2>Control in Neural Speech Synthesis</h2>
    <para>
      TTS is a typical one-to-many mapping problem, since multiple possible speech waveforms correspond to the same text message, owing to variations in
      pitch, duration, volume, energy and prosody. Controlling these parameters is necessary, as the model is prone to overfit to the variations of the
target speech in the training set, resulting in poor generalization ability. In the following experiments, we model pitch, and emotion, two features which influence the quality of speech explicity. We then add this variance information to
      the phoneme hidden sequence before decoding. We look through the attention values to see how they change.
    </para>
    <h3>Pitch</h3>
    Pitch is a key feature to convey emotion, and greatly affects the speech prosody.  FastSpeech provides a way to control pitch by adding pitch variance information to the phoneme hidden sequence. The ground truth pitch information, extracted from speech waveforms, are used as targets during training. The following figure shows how attention changes after adding pitch embeddings.
    <h3>Emotion</h3>
    We now experiment with another parameter, Emotion, to control speech output. We add emotion embeddings to the phoneme hidden sequence, and notice the difference in output speech waveform and attention of each token to input.
    <h3>Citations</h3>
    <p>
      <d-slider style="width: 200px;"></d-slider>
    </p>
    <p>We can<d-cite bibtex-key="mercier2011humans"></d-cite> also cite <d-cite
        key="gregor2015draw,mercier2011humans,openai2018charter"></d-cite> external publications. <d-cite
        key="dong2014image,dumoulin2016guide,mordvintsev2015inceptionism"></d-cite>. We should also be testing footnotes
      <d-footnote>This will become a hoverable footnote. This will become a hoverable footnote. This will become a
        hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote. This will
        become a hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote.
      </d-footnote>. There are multiple footnotes, and they appear in the appendix<d-footnote>Given I have coded them
        right. Also, here's math in a footnote: <d-math>c = \sum_0^i{x}</d-math>. Also, a citation. Box-ception<d-cite
          key='gregor2015draw'></d-cite>!</d-footnote> as well.</p>
    <h3>Checking attenton values with and without pitch embeddings</h3>
    <figure style="grid-column: page; margin: 1rem 0; align-content: center">
      <iframe src="head_view_bert_no_pitch.html" width="500" height="1000" zoom="3" align="left"></iframe>
      <iframe src="head_view_bert_pitch.html" width="500" height="1000" zoom="3" align="right"></iframe>
    </figure>
    <h3>Checking attenton values with and without emotion embeddings</h3>
    <figure style="grid-column: page; margin: 1rem 0; align-content: center">
      <iframe src="head_view_bert_no_pitch.html" width="500" height="1000" zoom="3" align="left"></iframe>
      <iframe src="head_view_bert_emotion.html" width="500" height="1000" zoom="3" align="right"></iframe>
    </figure>

  </d-article>

  <d-appendix>

    <h3>Contributions</h3>
    <p>Some text describing who did what.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

</body>
