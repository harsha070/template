<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
</head>

<body>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Control in Neural Speech Synthesis",
    "description": "",
    "authors": [
      {
        "author":"Sree Harsha Tanneru",
        "authorURL":"https://harsha070.github.io/",
        "affiliations": [{"name": "Independent"}]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <!--<figure style="grid-column: page; margin: 1rem 0;"><img src="momentum.png"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>-->
    <p>Speech Synthesis, also known as Text to Speech (TTS) is the task of synthesizing natural and intelligible speech from text, with broad applications in Industry. Modern day TTS Systems involve expertise from Machine Learning, Linguistics and Signal Processing. Research on TTS has shifted from Early Concatenative Synthesis, to Parametric Synthesis to Neural Speech Synthesis. In this blog post, we give a high level overview of TTS Systems, and illustrate Why and How Speech Output can be controlled. We specifically look at how and attention varies with the explicit introduction of pitch and emotion features.</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>Overview of Neural Text-to-Speech Synthesis</h2>
    <para>With the development of Deep Learning, Neural Network based TTS Approaches have become prominent. Neural Speech Synthesis approaches give higher voice quality in terms of both intelligibility and naturalness, and less requirement on human preprocessing and feature development. A Modern TTS System consists of three components - Text Analysis Module, Acoustic Model and a Vocoder. Letâ€™s look into each in detail.</para>
    <h3>Text Analysis</h3>
    <p>Text Analysis converts text into rich lingusitc information about pronunciation, rhythm, stress, intonation and prosody. These lingusitc features serve as the input to acoustic models.
    Phonemes are one such way to repesent linguistic features. Phoneme is a perceptually distinct unit of sound.
      English language has 44 phones, meaning pronunciation of any english word can be represented with these 44 phones.
        <table style="width:70%" class="center">
          <tr>
            <th>Word</th>
            <th>Phoneme Representation</th>
          </tr>
          <tr>
            <td>The</td>
            <td><b>DH AH0</b></td>
          </tr>
          <tr>
            <td>Quick</td>
            <td>K W IH1 K</td>
          </tr>
          <tr>
            <td>Brown</td>
            <td>B R AW1 N</td>
          </tr>
        </table>
    </p>
    <figure style="grid-column: page; margin: 0rem 0;"><img src="text-analysis.gif"
        style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure>
    <h3>Acoustic Model</h3>
    <p>Acoustic Models are Sequence-To-Sequence Models which generate Acoustic features from linguistic features or directly from raw text. The Acoustic features are later converted into a speech waveform. The kind of acoustic features determine the type of architecture used for generation. Mel-Spectrogram is a widely used acoustic feature output, owing to its minimal information loss.
      Mel-Spectrogram is derived based on linear cosine transform of speech wave on a log linear frequency scale.</p>
    <figure style="grid-column: page; margin: 0rem 0;"><img src="ezgif.com-gif-maker.gif"
        style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure>
    <h3>Vocoders</h3>
    <p>Vocoders are used to synthesize waveform from acoustic features. Neural Vocoders have recently gained popularity owing to their higher speech quality.
     Since speech waveform is very long, autoregressive waveform generation takes much inference time. Thus, generative models such as Flow, GAN,
    VAE, and DDPM (Denoising Diffusion Probabilistic Model, Diffusion for short) are used in waveform generation. Prominent TTS Methods used GAN based Vocoders, where Generators use transposed and dilated convolutions
      to generate the waveform, and Discriminator judges the authenticity of generated data.</p>
    <figure style="grid-column: span; margin: 0rem 0; align-content: center"><img src="vocoder.png"
        style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure>
    <a class="marker" href="#section-0" id="section-0"><span>0</span></a>
    <h2>Why Control ?</h2>
    <para>
      In a traditional Machine Learning problem, generalization is typically achieved by throwing in more data. Unlike a traditional machine learning problem, Text-To-Speech is a one-to-many mapping problem. TTS is a typical one-to-many mapping problem, since multiple possible speech waveforms correspond to the same text message, owing to variations in pitch, duration, volume, energy and prosody. Controlling these parameters is necessary, as the model is prone to overfit to the variations of the target speech in the training set, resulting in poor generalization ability. Recent Research has demonstrated ability to control Duration, Pitch, Energy in the output speech. In the following experiments, we model pitch, and emotion, two features which influence the quality of speech explicitly. We then add this variance information to the phoneme hidden sequence before decoding. We look through the attention values to see how they affect the output.
    </para>
    <h3>Pitch</h3>
    Pitch is a key feature to convey emotion, and greatly affects the speech prosody.  FastSpeech provides a way to control pitch by adding pitch variance information to the phoneme hidden sequence. The ground truth pitch information, extracted from speech waveforms, are used as targets during training. The following figure shows how attention changes after adding pitch embeddings.
    <h3>Emotion</h3>
    We now experiment with another parameter, Emotion, to control speech output. We add emotion embeddings to the phoneme hidden sequence, and notice the difference in output speech waveform and attention of each token to input.
    <h3>Checking attenton values with and without pitch embeddings</h3>
    <aside>
      Hover over any token on the left/right side of the visualization to filter attention from/to that token. The colors correspond to different attention heads.
      Double-click on any of the colored tiles at the top to filter to the corresponding attention head.
      Single-click on any of the colored tiles to toggle selection of the corresponding attention head.
      Click on the Layer drop-down to change the model layer (zero-indexed).
      The lines show the attention from each token (left) to every other token (right). Darker lines indicate higher attention weights. When multiple heads are selected, the attention weights are overlaid on one another.
    </aside>
    <figure style="grid-column: page; margin: 1rem 0; align-content: center">
      <iframe src="head_view_bert_no_pitch.html" width="500" height="1000" zoom="3" align="left"></iframe>
      <iframe src="head_view_bert_pitch.html" width="500" height="1000" zoom="3" align="right"></iframe>
    </figure>
    <figcaption align="left">Attention without pitch embeddings</figcaption>
    <figcaption align="right">Attention without pitch embeddings</figcaption>
    <h3>Checking attenton values with and without emotion embeddings</h3>
    <figure style="grid-column: page; margin: 1rem 0; align-content: center">
      <iframe src="head_view_bert_no_pitch.html" width="500" height="1000" zoom="3" align="left"></iframe>
      <iframe src="head_view_bert_emotion.html" width="500" height="1000" zoom="3" align="right"></iframe>
    </figure>
    <figcaption align="left">Attention without Emotion embedding</figcaption>
    <figcaption align="right">Attention with Emotion embedding</figcaption>

    <h3>Citations</h3>
    <p>
      <d-slider style="width: 200px;"></d-slider>
    </p>
    <p>We can<d-cite bibtex-key="mercier2011humans"></d-cite> also cite <d-cite
        key="gregor2015draw,mercier2011humans,openai2018charter"></d-cite> external publications. <d-cite
        key="dong2014image,dumoulin2016guide,mordvintsev2015inceptionism"></d-cite>. We should also be testing footnotes
      <d-footnote>This will become a hoverable footnote. This will become a hoverable footnote. This will become a
        hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote. This will
        become a hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote.
      </d-footnote>. There are multiple footnotes, and they appear in the appendix<d-footnote>Given I have coded them
        right. Also, here's math in a footnote: <d-math>c = \sum_0^i{x}</d-math>. Also, a citation. Box-ception<d-cite
          key='gregor2015draw'></d-cite>!</d-footnote> as well.</p>

  </d-article>

  <d-appendix>

    <h3>Contributions</h3>
    <p>Harsha is the sole contributor of this blog.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

</body>
